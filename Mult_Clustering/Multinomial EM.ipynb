{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "from scipy import sparse\n",
    "\n",
    "NUM_SAMPLES = 200\n",
    "LARGE_VAL = 10000000\n",
    "ITER_THRESH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to get the 20 newsgroup data\n",
    "def get_data():\n",
    "    #news_groups_all = fetch_20newsgroups(subset='all')\n",
    "    #news_data = news_groups_all.data\n",
    "\n",
    "    #Load in the vectorized news group data from scikit-learn package\n",
    "    news = fetch_20newsgroups(subset='all')\n",
    "    all_data = np.array(news.data)\n",
    "    all_targets = np.array(news.target)\n",
    "    class_names = news.target_names\n",
    "\n",
    "    #Set class pairings as described in the multiview clustering paper\n",
    "    view1_classes = ['comp.graphics','rec.motorcycles', 'sci.space', 'rec.sport.hockey', 'comp.sys.ibm.pc.hardware']\n",
    "    view2_classes = ['rec.autos', 'sci.med','misc.forsale', 'soc.religion.christian','comp.os.ms-windows.misc']\n",
    "    \n",
    "    #Create lists to hold data and labels for each of the 5 classes across 2 different views\n",
    "    labels =  [num for num in range(len(view1_classes)) for _ in range(NUM_SAMPLES)]\n",
    "    labels = np.array(labels)\n",
    "    view1_data = list()\n",
    "    view2_data = list()\n",
    "    \n",
    "    #Randomly sample 200 items from each of the selected classes in view1\n",
    "    for ind in range(len(view1_classes)):\n",
    "        class_num = class_names.index(view1_classes[ind])\n",
    "        class_data = all_data[(all_targets == class_num)]\n",
    "        indices = np.random.choice(class_data.shape[0], NUM_SAMPLES)\n",
    "        view1_data.append(class_data[indices])\n",
    "    view1_data = np.concatenate(view1_data)\n",
    "   \n",
    "        \n",
    "    #Randomly sample 200 items from each of the selected classes in view2\n",
    "    for ind in range(len(view2_classes)):\n",
    "        class_num = class_names.index(view2_classes[ind])\n",
    "        class_data = all_data[(all_targets == class_num)]\n",
    "        indices = np.random.choice(class_data.shape[0], NUM_SAMPLES)\n",
    "        view2_data.append(class_data[indices])  \n",
    "    view2_data = np.concatenate(view2_data)\n",
    "    \n",
    "    #Vectorize the data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    view1_data = vectorizer.fit_transform(view1_data)\n",
    "    view2_data = vectorizer.fit_transform(view2_data)\n",
    "\n",
    "    #Shuffle and normalize vectors\n",
    "    shuffled_inds = np.random.permutation(NUM_SAMPLES * len(view1_classes))\n",
    "    view1_data = sparse.vstack(view1_data)\n",
    "    view2_data = sparse.vstack(view2_data)\n",
    "    view1_data = np.array(view1_data[shuffled_inds].todense())\n",
    "    view2_data = np.array(view2_data[shuffled_inds].todense())\n",
    "    labels = labels[shuffled_inds]\n",
    "\n",
    "    return view1_data, view2_data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_data, v2_data, labels = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(data, w_probs, alphas, k):\n",
    "    likes = list()\n",
    "    for ind in range(k):\n",
    "        like = np.power(w_probs[ind], data)\n",
    "        like = np.prod(like, axis=1)\n",
    "        likes.append(like)  \n",
    "    likes = np.vstack(likes)\n",
    "    likes_p = likes * alphas.reshape((-1, 1))\n",
    "    likes_sum = np.sum(likes_p, axis=1).reshape((-1, 1))\n",
    "    likes_sum[likes_sum == 0] = 1\n",
    "    posterior = likes_p / likes_sum\n",
    "    log_like = np.sum(np.log2(likes_sum))\n",
    "    return posterior, log_like\n",
    "\n",
    "def compute_posterior2(data, w_probs, alphas, k):\n",
    "    likes = list()\n",
    "    for ind in range(k):\n",
    "        like = data * w_probs[ind]\n",
    "        like = np.sum(like, axis=1)\n",
    "        likes.append(like)  \n",
    "    likes = np.vstack(likes)\n",
    "    likes_p = likes * alphas.reshape((-1, 1))\n",
    "    likes_sum = np.sum(likes_p, axis=1).reshape((-1, 1))\n",
    "    likes_sum[likes_sum == 0] = 1\n",
    "    posterior = likes_p / likes_sum\n",
    "    log_like = np.sum(np.log2(likes_sum))\n",
    "    return posterior, log_like\n",
    "\n",
    "def iterate(data, posteriors, k):\n",
    "    \n",
    "    #For each of the mixture components, compute model params\n",
    "    w_probs = list()\n",
    "    for ind in range(k):\n",
    "        numer = data * posteriors[ind].reshape((-1, 1))\n",
    "        numer = 1 + np.sum(numer, axis=0)\n",
    "        denom = np.sum(numer)\n",
    "        if(denom == 0):\n",
    "            denom = 1\n",
    "        probs = numer/denom\n",
    "        w_probs.append(probs)\n",
    "    w_probs = np.vstack(w_probs)\n",
    "    alphas = np.mean(posteriors, axis=1)\n",
    "\n",
    "    #Compute new posterior\n",
    "    new_posteriors, log_like = compute_posterior(data, w_probs, alphas, k)\n",
    "    return w_probs, alphas, new_posteriors, log_like\n",
    "\n",
    "def final_clusters(posteriors):\n",
    "    metric = posteriors[0] + posteriors[1]\n",
    "    f_clusters = np.argmax(metric, axis = 0)\n",
    "    return f_clusters\n",
    "\n",
    "def compute_entropy(partitions, labels, k, num_classes):\n",
    "    \n",
    "    total_entropy = 0\n",
    "    num_examples = partitions.shape[0]\n",
    "    for part in range(k):\n",
    "        labs = labels[partitions == part]\n",
    "        part_size = labs.shape[0]\n",
    "        part_entropy = 0\n",
    "        for cl in range(num_classes):\n",
    "            prop = np.sum(labs == cl) * 1.0 / part_size\n",
    "            ent = 0\n",
    "            if(prop != 0):\n",
    "                ent = - prop * np.log2(prop)\n",
    "            part_entropy += ent\n",
    "        part_entropy = part_entropy * part_size / num_examples\n",
    "        total_entropy += part_entropy\n",
    "    return total_entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main kmeans clustering algorithm\n",
    "def multinomial(v_data, labels, k = 5):\n",
    "\n",
    "    #Initialize cluster centers, partitions, and loop params\n",
    "    w_probs2 = np.random.random((k, v_data[1].shape[1]))\n",
    "    w_probs2 /= np.linalg.norm(w_probs2, axis=1).reshape((-1, 1))\n",
    "    w_probs = [None, w_probs2]                              \n",
    "    alphas2 = (1/k) * np.ones((k,))\n",
    "    alphas = [None, alphas2]\n",
    "    \n",
    "    posterior2, log_likes = compute_posterior(v_data[1], w_probs[1], alphas[1], k)\n",
    "    posteriors = [None, posterior2]\n",
    "    objective = [0, 0]\n",
    "    iter_stall = 0\n",
    "    iter_num = 0\n",
    "    entropy = 0\n",
    "    \n",
    "    while(iter_stall < ITER_THRESH):\n",
    "        iter_num += 1\n",
    "        view = (iter_num + 1) % 2\n",
    "        \n",
    "        #Switch partitions, Maximization, and Expectation\n",
    "        w_probs[view], alphas[view], posteriors[view], log_like = iterate(v_data[view], posteriors[(view + 1) % 2], k)\n",
    "        iter_stall += 1\n",
    "        #Recompute objective function\n",
    "        if(log_like > objective[view]):\n",
    "            objective[view] = log_like\n",
    "            iter_stall = 0\n",
    "\n",
    "        #Obtain evaluation metrics\n",
    "        f_clusters = final_clusters(posteriors)\n",
    "        entropy = compute_entropy(f_clusters, labels, k, 5)\n",
    "\n",
    "    return entropy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "ent = multinomial([v1_data, v2_data], labels, 5)\n",
    "print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
