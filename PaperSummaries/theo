# A Survey on Multi-view Learning
### Chang Xu, Dacheng Tao, Chao Xu
link: https://arxiv.org/abs/1304.5634

# Summary

Multi-view data is data from multiple sources or different feature subsets. Three groups of multi-view learning approaches are (1) co-training, (2) multiple kernel learning and (3) subspace learning. Co-training algorithms train alternately to maximize the mutual agreement on two distinct views of the data. Multiple kernel learning algorithms combine kernels that correspond to different views to improve performance. Subspace learning algorithms generate a latent subspaces shared by multiple views by assuming that the input views are generated from this latent subspace. These approaches all exploit either the consensus principle or complementary principle. 

Conventional ML algorithms concatenate all multiple views into a single view to adapt to the learning setting. This causes over-fitting and is not meaningful because each view has a specific statistical property. Multi-view learning introduces a function to model a particular view and jointly optimizes all functions to exploit the redundant views of the same input data and improve the learning performance.

Success of co-training algorithms relies on (a) sufficiency- each view is sufficient for classification on its own, (b) compatibility- the target function of both views predict the same label for co-occurring features with a high probability and (c) conditional independence- views are conditionally independent given the label. 

Consensus principle aims to maximize the agreement on multiple distinct views. Complementary principle states that in a multi-view setting, each view of the data may contain some knowledge that other views don’t have. Therefore, multiple views can be employed to comprehensively and accurately describe the data.

Priority for multi-view learning is the acquisition of redundant views. Multiple view generation not only obtains the views of different attributes, but also involves ensuring the views sufficiently represent the data and satisfy the assumptions required for learning.

Instead of providing a single representative set of features, feature set partitioning decomposes the original set of features into multiple disjoint subsets to construct each view. Simple way to convert from single to multiple views is randomly split original feature set into different views at random. Sub setting feature set in a way that adheres to the multi-view learning paradigm is not trivial—depends on chosen learner and the data domain. Look into random subspace method (RSM), Pseudo Multi-view Co-training (PMC), genetic algorithms (GAs), several kernel functions. View construction methods can be categorized into 3 classes. (1) construct MV from meta data through random approaches. (2) reshape or decompose the original single-view feature into multiple views (matrix representations or kernel functions). (3) methods that perform feature set partitioning automatically such as PMC. In MV feature selection, the relationships between MV should be considered besides the information within each view.

After constructing multiple views, need to evaluate these views and ensure effectiveness for MV learning algorithms. Approaches that analyze relationships between multiple views.

Co-training style algorithms usually train separate but correlated learners on each view and outputs of learners are forced to be similar on same validation points. Under consensus principle, the goal of each iteration is to max the consistency of 2 learners on the validation set. Maximizing agreement on predictions of two classifiers on the labeled dataset and minimizing the disagreement on the predictions of two classifiers on unlabeled dataset. Classifiers learn from each other and reach an optimal solution.

Co-training proposed for semi-supervised learning—access to labeled and unlabeled data. Each example can be partitioned into two distinct views and makes 3 assumptions for success- sufficiency, compatibility, and conditional independence.

