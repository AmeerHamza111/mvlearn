{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import dot, eye, ones, zeros\n",
    "import scipy.linalg\n",
    "from kernel_icd import kernel_icd\n",
    "from kernels import LinearKernel\n",
    "\n",
    "class KCCA(object):\n",
    "    \"\"\"An implementation of Kernel Canonical Correlation Analysis.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel1, kernel2, regularization, method = 'kettering_method',\n",
    "                 decomp = 'full', lrank = None,\n",
    "                 scaler1 = None,\n",
    "                 scaler2 = None,\n",
    "                 max_variance_ratio = 1.0):\n",
    "\n",
    "        if decomp not in ('full', 'icd'): #error handling\n",
    "            raise ValueError(\"Error: valid decom values are full or icd, received: \"+str(decomp))\n",
    "            \n",
    "        #initializing values inputted to class\n",
    "        self.kernel1 = kernel1\n",
    "        self.kernel2 = kernel2\n",
    "        self.reg = regularization\n",
    "        self.method = getattr(self, decomp + \"_\" + method)\n",
    "            #getattr() returns value of named attribute\n",
    "            #equivalent to self.method = self.decomp+'_'+self.method\n",
    "\n",
    "        self.decomp = decomp\n",
    "        self.lrank = lrank\n",
    "\n",
    "        self.alpha1 = None\n",
    "        self.alpha2 = None\n",
    "        self.trainX1 = None\n",
    "        self.trainX2 = None\n",
    "        self.max_variance_rato = max_variance_ratio\n",
    "\n",
    "        if scaler1 is not None:\n",
    "            if hasattr(scaler1, \"transform\"):  #sklearn scaler\n",
    "                # hasattr returns true/false if variable has an attribute\n",
    "                self.scaler1 = scaler1.transform\n",
    "            else:  #assume callable function\n",
    "                self.scaler1 = scaler1\n",
    "        else:\n",
    "            self.scaler1 = None\n",
    "\n",
    "        if scaler2 is not None:\n",
    "            if hasattr(scaler2, \"transform\"):  #sklearn scaler\n",
    "                self.scaler2 = scaler2.transform\n",
    "            else:  #assume callable function\n",
    "                self.scaler2 = scaler2\n",
    "        else:\n",
    "            self.scaler2 = None\n",
    "\n",
    "    def full_standard_hardoon_method(self, K1, K2, reg):\n",
    "\n",
    "        N = K1.shape[0]\n",
    "        I = eye(N)\n",
    "        Z = numpy.zeros((N,N))\n",
    "\n",
    "        R1 = numpy.c_[Z, dot(K1, K2)]\n",
    "        R2 = numpy.c_[dot(K2, K1), Z]\n",
    "        R =  numpy.r_[R1, R2]\n",
    "\n",
    "        D1 = numpy.c_[dot(K1, K1 + reg*I), Z]\n",
    "        D2 = numpy.c_[Z, dot(K2, K2 + reg*I)]\n",
    "        D = 0.5*numpy.r_[D1, D2]\n",
    "\n",
    "        return (R, D)\n",
    "\n",
    "    def full_simplified_hardoon_method(self, K1, K2, reg):\n",
    "\n",
    "        N = K1.shape[0]\n",
    "        I = eye(N)\n",
    "        Z = numpy.zeros((N,N))\n",
    "\n",
    "        R1 = numpy.c_[Z, K2]\n",
    "        R2 = numpy.c_[K1, Z]\n",
    "        R =  numpy.r_[R1, R2]\n",
    "\n",
    "        D1 = numpy.c_[K1 + reg*I, Z]\n",
    "        D2 = numpy.c_[Z, K2 + reg*I]\n",
    "        D = numpy.r_[D1, D2]\n",
    "\n",
    "        return (R, D)\n",
    "\n",
    "    def full_kettering_method(self, K1, K2, reg):\n",
    "\n",
    "        N = K1.shape[0]\n",
    "        I = eye(N)\n",
    "        Z = numpy.zeros((N,N))\n",
    "\n",
    "        R1 = numpy.c_[K1, K2]\n",
    "        R2 = R1\n",
    "        R = 1./2 * numpy.r_[R1, R2]\n",
    "\n",
    "        D1 = numpy.c_[K1 + reg*I, Z]\n",
    "        D2 = numpy.c_[Z, K2 + reg*I]\n",
    "        D = numpy.r_[D1, D2]\n",
    "\n",
    "        return (R, D)\n",
    "\n",
    "    #def kcca(self, K1, K2):\n",
    "\n",
    "        ##remove the mean in features space\n",
    "        #N = K1.shape[0]\n",
    "        #N0 = eye(N) - 1./N * ones(N)\n",
    "\n",
    "        #if self.scaler1 is None:\n",
    "            #K1 = dot(dot(N0,K1),N0)\n",
    "        #if self.scaler2 is None:\n",
    "            #K2 = dot(dot(N0,K2),N0)\n",
    "\n",
    "        #R, D = self.method(K1, K2, self.reg)\n",
    "\n",
    "        ##solve generalized eigenvalues problem\n",
    "        #betas, alphas = scipy.linalg.eig(R,D)\n",
    "        #ind = numpy.argsort(numpy.real(betas))\n",
    "        #max_ind = ind[-1]\n",
    "        #alpha = alphas[:, max_ind]\n",
    "        #alpha = alpha/numpy.linalg.norm(alpha)\n",
    "        #beta = numpy.real(betas[max_ind])\n",
    "\n",
    "        #alpha1 = alpha[:N]\n",
    "        #alpha2 = alpha[N:]\n",
    "\n",
    "        #y1 = dot(K1, alpha1)\n",
    "        #y2 = dot(K2, alpha2)\n",
    "\n",
    "        #self.alpha1 = alpha1\n",
    "        #self.alpha2 = alpha2\n",
    "\n",
    "        #return (y1, y2, beta)\n",
    "        \n",
    "    #******* .fit calls kcca ********************\n",
    "    def kcca(self, K1, K2):\n",
    "        #K1 and K2 are the two linearkernels of the two dataviews\n",
    "\n",
    "        #remove the mean in features space\n",
    "        N = K1.shape[0]\n",
    "        N0 = eye(N) - 1./N * ones(N) #./ ensures division yields a float\n",
    "        \n",
    "        #scaler1 and scaler2 hold the datasets\n",
    "        if self.scaler1 is None:\n",
    "            K1 = dot(dot(N0,K1),N0)\n",
    "        if self.scaler2 is None:\n",
    "            K2 = dot(dot(N0,K2),N0)\n",
    "\n",
    "        R, D = self.method(K1, K2, self.reg) #calls one of the functions above\n",
    "\n",
    "        #solve generalized eigenvalues problem\n",
    "        betas, alphas = scipy.linalg.eig(R,D)\n",
    "\n",
    "        #sorting according to eigenvalue\n",
    "        betas =  numpy.real(betas)\n",
    "        ind = numpy.argsort(betas)\n",
    "        betas = betas[ind]\n",
    "        betas = betas[::-1]\n",
    "\n",
    "        #finding the components\n",
    "        if self.max_variance_rato < 1.0:\n",
    "            n_samples = len(betas)\n",
    "            explained_variance = (betas ** 2) / n_samples\n",
    "            explained_variance_ratio = explained_variance / explained_variance.sum()\n",
    "            ratio_cumsum = explained_variance_ratio.cumsum()\n",
    "            n_components = numpy.sum(ratio_cumsum < self.max_variance_rato) + 1\n",
    "        else:\n",
    "            #using all the dimensions\n",
    "            n_components = len(betas)\n",
    "            \n",
    "        alphas = alphas[:, ind]\n",
    "        alpha = alphas[:, :n_components]\n",
    "\n",
    "        #alpha = alpha/numpy.linalg.norm(alpha)\n",
    "        #making unit vectors\n",
    "        alpha = alpha / (numpy.sum(numpy.abs(alpha)**2 ,axis=0)**(1./2))\n",
    "\n",
    "        alpha1 = alpha[:N, :]\n",
    "        alpha2 = alpha[N:, :]\n",
    "\n",
    "        y1 = dot(K1, alpha1)\n",
    "        y2 = dot(K2, alpha2)\n",
    "\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "\n",
    "        return (y1, y2, betas[0])\n",
    "\n",
    "    def icd_simplified_hardoon_method(self, G1, G2, reg):\n",
    "        N1 = G1.shape[1]\n",
    "        N2 = G2.shape[1]\n",
    "\n",
    "        Z11 = zeros((N1, N1))\n",
    "        Z22 = zeros((N2, N2))\n",
    "        Z12 = zeros((N1,N2))\n",
    "\n",
    "        I11 = eye(N1)\n",
    "        I22 = eye(N2)\n",
    "\n",
    "        R1 = numpy.c_[Z11, dot(G1.T, G2)]\n",
    "        R2 = numpy.c_[dot(G2.T, G1), Z22]\n",
    "        R =  numpy.r_[R1, R2]\n",
    "\n",
    "        D1 = numpy.c_[dot(G1.T, G1) + reg*I11, Z12]\n",
    "        D2 = numpy.c_[Z12.T, dot(G2.T, G2) + reg*I22]\n",
    "        D = numpy.r_[D1, D2]\n",
    "\n",
    "        return (R, D)\n",
    "\n",
    "    def icd(self, G1, G2):\n",
    "        \"\"\"Incomplete Cholesky decomposition\n",
    "        \"\"\"\n",
    "\n",
    "        # remove mean. avoid standard calculation N0 = eye(N)-1/N*ones(N);\n",
    "        G1 = G1 - numpy.array(numpy.mean(G1, 0), ndmin=2, copy=False)\n",
    "        G2 = G2 - numpy.array(numpy.mean(G2, 0), ndmin=2, copy=False)\n",
    "\n",
    "        R, D = self.method(G1, G2, self.reg)\n",
    "\n",
    "        #solve generalized eigenvalues problem\n",
    "        betas, alphas = scipy.linalg.eig(R,D)\n",
    "        ind = numpy.argsort(numpy.real(betas))\n",
    "        max_ind = ind[-1]\n",
    "        alpha = alphas[:, max_ind]\n",
    "        alpha = alpha/numpy.linalg.norm(alpha)\n",
    "        beta = numpy.real(betas[max_ind])\n",
    "\n",
    "        N1 = G1.shape[1]\n",
    "        alpha1 = alpha[:N1]\n",
    "        alpha2 = alpha[N1:]\n",
    "\n",
    "        y1 = dot(G1, alpha1)\n",
    "        y2 = dot(G2, alpha2)\n",
    "\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "\n",
    "        return (y1, y2, beta)\n",
    "\n",
    "    #************ this is the function that will likely be called the most *********************\n",
    "    def fit(self, X1, X2):\n",
    "\n",
    "        if self.scaler1 is not None:\n",
    "            X1 = self.scaler1(X1) #scaler1 is a lambda function that copies the input\n",
    "        if self.scaler2 is not None:\n",
    "            X2 = self.scaler2(X2) #scaler2 does the same thing as scaler1\n",
    "        \n",
    "        #initialize training sets\n",
    "        self.trainX1 = X1\n",
    "        self.trainX2 = X2\n",
    "\n",
    "        if self.decomp == \"full\":\n",
    "            self.K1 = self.kernel1(X1, X1) #kernel1 is the linearkernel class\n",
    "            self.K2 = self.kernel2(X2, X2) #kernel2 is the linearkernel class\n",
    "            (y1, y2, beta) = self.kcca(self.K1, self.K2) #kcca function is called\n",
    "        else:\n",
    "            # get incompletely decomposed kernel matrices. K \\approx G*G'\n",
    "            self.K1 = kernel_icd(X1, self.kernel1,  self.lrank)\n",
    "            self.K2 = kernel_icd(X2, self.kernel2,  self.lrank)\n",
    "            (y1, y2, beta) = self.icd(self.K1, self.K2) #icd function is called\n",
    "\n",
    "        self.y1_ = y1\n",
    "        self.y2_ = y2\n",
    "        self.beta_ = beta\n",
    "        return self\n",
    "\n",
    "    def transform(self, X1 = None, X2 = None):\n",
    "        \"\"\"\n",
    "\n",
    "        Features centering taken from:\n",
    "        Scholkopf, B., Smola, A., & Muller, K. R. (1998).\n",
    "        Nonlinear component analysis as a kernel eigenvalue problem.\n",
    "        Neural computation, 10(5), 1299-1319.\n",
    "        \"\"\"\n",
    "        \n",
    "        rets = []\n",
    "        if X1 is not None:\n",
    "\n",
    "            if self.scaler1 is not None:\n",
    "                X1 = self.scaler1(X1)\n",
    "\n",
    "            Ktest = self.kernel1(X1, self.trainX1)\n",
    "            K = self.K1\n",
    "\n",
    "            if self.scaler1 is None:\n",
    "                L, M = Ktest.shape\n",
    "                ones_m = ones((M, M))\n",
    "                ones_mp = ones((L, M)) / M\n",
    "\n",
    "                #features centering\n",
    "                K1 = (Ktest - dot(ones_mp, K)\n",
    "                      - dot(Ktest, ones_m) + dot(dot(ones_mp, K), ones_m)\n",
    "                      )\n",
    "            else:\n",
    "                K1 = Ktest\n",
    "\n",
    "            res1 =  dot(K1, self.alpha1)\n",
    "            rets.append(res1)\n",
    "\n",
    "        if X2 is not None:\n",
    "\n",
    "            if self.scaler2 is not None:\n",
    "                X2 = self.scaler2(X2)\n",
    "\n",
    "            Ktest = self.kernel2(X2, self.trainX2)\n",
    "            K = self.K2\n",
    "\n",
    "            if self.scaler2 is None:\n",
    "                L, M = Ktest.shape\n",
    "                ones_m = ones((M, M))\n",
    "                ones_mp = ones((L, M)) / M\n",
    "\n",
    "                #features centering\n",
    "                K2 = (Ktest - dot(ones_mp, K)\n",
    "                      - dot(Ktest, ones_m) + dot(dot(ones_mp, K), ones_m)\n",
    "                      )\n",
    "            else:\n",
    "                K2 = Ktest\n",
    "\n",
    "            res2 =  dot(K2, self.alpha2)\n",
    "            rets.append(res2)\n",
    "\n",
    "        return rets\n",
    "\n",
    "def _mean_and_std(X, axis=0, with_mean=True, with_std=True):\n",
    "    \"\"\"Compute mean and std dev for centering, scaling\n",
    "\n",
    "    Zero valued std components are reset to 1.0 to avoid NaNs when scaling.\n",
    "    \"\"\"\n",
    "    X = numpy.asarray(X)\n",
    "    Xr = numpy.rollaxis(X, axis)\n",
    "\n",
    "    if with_mean:\n",
    "        mean_ = Xr.mean(axis=0)\n",
    "    else:\n",
    "        mean_ = None\n",
    "\n",
    "    if with_std:\n",
    "        std_ = Xr.std(axis=0)\n",
    "        if isinstance(std_, numpy.ndarray):\n",
    "            std_[std_ == 0.0] = 1.0\n",
    "        elif std_ == 0.:\n",
    "            std_ = 1.\n",
    "    else:\n",
    "        std_ = None\n",
    "\n",
    "    return mean_, std_\n",
    "\n",
    "class UnscaledKCCA(KCCA):\n",
    "    def __init__(self, kernel1, kernel2, regularization,\n",
    "                 method = 'kettering_method',\n",
    "                 max_variance_ratio = 1.0,\n",
    "                 ) :\n",
    "        super(UnscaledKCCA, self).__init__(kernel1, kernel2, regularization,\n",
    "                 method,\n",
    "                 'full', None,\n",
    "                 None,\n",
    "                 None,\n",
    "                 max_variance_ratio)\n",
    "\n",
    "        #this is to avoid pickling problems\n",
    "        self.method = method\n",
    "\n",
    "    def kcca(self, K1, K2):\n",
    "\n",
    "        method = getattr(self, \"full_\" + self.method)\n",
    "        R, D = method(K1, K2, self.reg)\n",
    "\n",
    "        #solve generalized eigenvalues problem\n",
    "        betas, alphas = scipy.linalg.eig(R,D)\n",
    "\n",
    "        #sorting according to eigenvalue\n",
    "        betas =  numpy.real(betas)\n",
    "        ind = numpy.argsort(betas)\n",
    "        betas = betas[ind]\n",
    "        betas = betas[::-1]\n",
    "\n",
    "        #fiding the components\n",
    "        n_samples = len(betas)\n",
    "        if self.max_variance_rato < 1.0:\n",
    "            explained_variance = (betas ** 2) / n_samples\n",
    "            explained_variance_ratio = explained_variance / explained_variance.sum()\n",
    "            ratio_cumsum = explained_variance_ratio.cumsum()\n",
    "            n_components = numpy.sum(ratio_cumsum < self.max_variance_rato) + 1\n",
    "        else:\n",
    "            #using all the dimensions\n",
    "            n_components = n_samples\n",
    "\n",
    "        alphas = alphas[:, ind]\n",
    "        alpha = alphas[:, :n_components]\n",
    "\n",
    "        #alpha = alpha/numpy.linalg.norm(alpha)\n",
    "        #making unit vectors\n",
    "        alpha = alpha / (numpy.sum(numpy.abs(alpha)**2 ,axis=0)**(1./2))\n",
    "\n",
    "        N = K1.shape[0]\n",
    "        alpha1 = alpha[:N, :] #up to shape of K1\n",
    "        alpha2 = alpha[N:, :] #remainder of alpha\n",
    "\n",
    "        y1 = dot(K1, alpha1)\n",
    "        y2 = dot(K2, alpha2)\n",
    "\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.y1_ = y1 #dot product of K1 and alpha1\n",
    "        self.y2_ = y2\n",
    "        self.beta_ = betas[0]\n",
    "        self.betas_ = betas\n",
    "\n",
    "        return (y1, y2, betas[0])\n",
    "\n",
    "    def fit(self, X1, X2, K1_args = None, K2_args=None):\n",
    "        self.trainX1 = X1\n",
    "        self.trainX2 = X2\n",
    "\n",
    "        if K1_args is not None:\n",
    "            self.K1 = self.kernel1(X1, X1, K1_args)\n",
    "        else:\n",
    "            self.K1 = self.kernel1(X1, X1)\n",
    "        if K2_args is not None:\n",
    "            self.K2 = self.kernel2(X2, X2, K2_args)\n",
    "        else:\n",
    "            self.K2 = self.kernel2(X2, X2)\n",
    "        (y1, y2, beta) = self.kcca(self.K1, self.K2)\n",
    "\n",
    "        self.y1_ = y1\n",
    "        self.y2_ = y2\n",
    "        self.beta_ = beta\n",
    "        return self\n",
    "\n",
    "    def transform(self, X1 = None, X2 = None,\n",
    "                  n_dims_frac = 0.1,\n",
    "                  K1_args = None,\n",
    "                  K2_args = None,):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        rets = []\n",
    "        if X1 is not None:\n",
    "            if type(n_dims_frac) is float:\n",
    "                n_dims = numpy.ceil(n_dims_frac * self.alpha1.shape[1])\n",
    "            else:\n",
    "                n_dims = n_dims_frac\n",
    "            if K1_args is not None:\n",
    "                Ktest = self.kernel1(X1, self.trainX1, K1_args)\n",
    "            else:\n",
    "                Ktest = self.kernel1(X1, self.trainX1)\n",
    "\n",
    "            res1 =  dot(Ktest, self.alpha1[:, :n_dims])\n",
    "            rets.append(res1)\n",
    "\n",
    "        if X2 is not None:\n",
    "            if type(n_dims_frac) is float:\n",
    "                n_dims = numpy.ceil(n_dims_frac * self.alpha2.shape[1])\n",
    "            else:\n",
    "                n_dims = n_dims_frac\n",
    "            if K2_args is not None:\n",
    "                Ktest = self.kernel2(X2, self.trainX2, K2_args)\n",
    "            else:\n",
    "                Ktest = self.kernel2(X2, self.trainX2)\n",
    "            K2 = self.K2\n",
    "\n",
    "            res2 =  dot(Ktest, self.alpha2[:, :n_dims])\n",
    "            rets.append(res2)\n",
    "\n",
    "        return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  0.9999999246853696\n",
      "Trying to test\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#sample implementation\n",
    "if __name__ == \"__main__\":\n",
    "    from kernels import DiagGaussianKernel\n",
    "    x1 = numpy.random.rand(20, 40) #100 x 20 matrix with rand values between 0 and 1\n",
    "    x2 = numpy.random.rand(20, 30) #100 x 30 matrix with rand values between 0 and 1\n",
    "    kernel = LinearKernel() #initialize a linear kernel class\n",
    "    cca = KCCA(kernel, kernel,\n",
    "                    regularization=1e-5,\n",
    "                    decomp='full',\n",
    "                    method='kettering_method',\n",
    "                    scaler1=lambda x:x,\n",
    "                    scaler2=lambda x:x).fit(x1,x2)\n",
    "\n",
    "    print(\"Done \",  cca.beta_) #prints the highest beta\n",
    "\n",
    "    orig_y1 = cca.y1_ #dot product of K1 and alpha1\n",
    "    orig_y2 = cca.y2_ #dot product of K2 and alpha2\n",
    "\n",
    "    print(\"Trying to test\")\n",
    "    y1, y2 = cca.transform(x1, x2) # calls transform\n",
    "    print(numpy.allclose(y1, orig_y1))\n",
    "    print(numpy.allclose(y2, orig_y2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
