{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the performance of Multi-View KMeans with Conditionally Independent Views on More Complex Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import scipy as scp\n",
    "from multiview.cluster.mv_k_means import MultiviewKMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating an artificial dataset where the conditional independence assumption between views holds\n",
    "\n",
    "Here, we create an artificial dataset where the conditional independence assumption between\n",
    "views, given the true labels, is enforced. Our artificial dataset is derived from the forest \n",
    "covertypes dataset from the scikit-learn package. This dataset is comprised of 7 different classes, with\n",
    "with 54 different numerical features per sample. To create our artificial data, we will select 500 samples from\n",
    "each of the first 6 classes in the dataset, and from these, construct 3 artificial classes with \n",
    "2 views each. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ci_data(num_samples=500):\n",
    "\n",
    "    #Load in the vectorized news group data from scikit-learn package\n",
    "    cov = fetch_covtype()\n",
    "    all_data = np.array(cov.data)\n",
    "    all_targets = np.array(cov.target)\n",
    "    \n",
    "    #Set class pairings as described in the multiview clustering paper\n",
    "    view1_classes = [1, 2, 3]\n",
    "    view2_classes = [4, 5, 6]\n",
    "    \n",
    "    #Create lists to hold data and labels for each of the classes across 2 different views\n",
    "    labels =  [num for num in range(len(view1_classes)) for _ in range(num_samples)]\n",
    "    labels = np.array(labels)\n",
    "    view1_data = list()\n",
    "    view2_data = list()\n",
    "    \n",
    "    #Randomly sample items from each of the selected classes in view1\n",
    "    for class_num in view1_classes:\n",
    "        class_data = all_data[(all_targets == class_num)]\n",
    "        indices = np.random.choice(class_data.shape[0], num_samples)\n",
    "        view1_data.append(class_data[indices])\n",
    "    view1_data = np.concatenate(view1_data)\n",
    "   \n",
    "        \n",
    "    #Randomly sample items from each of the selected classes in view2\n",
    "    for class_num in view2_classes:\n",
    "        class_data = all_data[(all_targets == class_num)]\n",
    "        indices = np.random.choice(class_data.shape[0], num_samples)\n",
    "        view2_data.append(class_data[indices])  \n",
    "    view2_data = np.concatenate(view2_data)\n",
    "\n",
    "    #Shuffle and normalize vectors\n",
    "    shuffled_inds = np.random.permutation(num_samples * len(view1_classes))\n",
    "    view1_data = np.vstack(view1_data)\n",
    "    view2_data = np.vstack(view2_data)\n",
    "    view1_data = view1_data[shuffled_inds]\n",
    "    view2_data = view2_data[shuffled_inds]\n",
    "    magnitudes1 = np.linalg.norm(view1_data, axis=0)\n",
    "    magnitudes2 = np.linalg.norm(view2_data, axis=0)\n",
    "    magnitudes1[magnitudes1 == 0] = 1\n",
    "    magnitudes2[magnitudes2 == 0] = 1\n",
    "    magnitudes1 = magnitudes1.reshape((1, -1))\n",
    "    magnitudes2 = magnitudes2.reshape((1, -1))\n",
    "    view1_data /= magnitudes1\n",
    "    view2_data /= magnitudes2\n",
    "    labels = labels[shuffled_inds]\n",
    "    return [view1_data, view2_data], labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a function to perform both single-view and multi-view kmeans clustering\n",
    "\n",
    "In the following function, we will perform single-view kmeans clustering on the two views separately and on them concatenated together. We also perform multi-view clustering using the multi-view algorithm. We will also compare the performance of multi-view and single-view versions of kmeans clustering. We will evaluate the purity of the resulting clusters from each algorithm with respect to the class labels using the normalized mutual information metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(seed, m_data, labels, n_clusters):\n",
    "    #################Single-view kmeans clustering#####################\n",
    "    # Cluster each view separately\n",
    "    s_kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=100)\n",
    "    s_clusters_v1 = s_kmeans.fit_predict(m_data[0])\n",
    "    s_clusters_v2 = s_kmeans.fit_predict(m_data[1])\n",
    "\n",
    "    # Concatenate the multiple views into a single view\n",
    "    s_data = np.hstack(m_data)\n",
    "    s_clusters = s_kmeans.fit_predict(s_data)\n",
    "\n",
    "    # Compute nmi between true class labels and single-view cluster labels\n",
    "    s_nmi_v1 = nmi_score(labels, s_clusters_v1)\n",
    "    s_nmi_v2 = nmi_score(labels, s_clusters_v2)\n",
    "    s_nmi = nmi_score(labels, s_clusters)\n",
    "    print('Single-view View 1 NMI Score: {0:.3f}\\n'.format(s_nmi_v1))\n",
    "    print('Single-view View 2 NMI Score: {0:.3f}\\n'.format(s_nmi_v2))\n",
    "    print('Single-view Concatenated NMI Score: {0:.3f}\\n'.format(s_nmi))\n",
    "\n",
    "    #################Multi-view kmeans clustering######################\n",
    "\n",
    "    # Use the MultiviewKMeans instance to cluster the data\n",
    "    m_kmeans = MultiviewKMeans(n_clusters=n_clusters, n_init=100, random_state=seed)\n",
    "    m_clusters = m_kmeans.fit_predict(m_data)\n",
    "\n",
    "    # Compute nmi between true class labels and multi-view cluster labels\n",
    "    m_nmi = nmi_score(labels, m_clusters)\n",
    "    print('Multi-view NMI Score: {0:.3f}\\n'.format(m_nmi))\n",
    "    \n",
    "    return m_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the Performance of Multi-view and Single-view KMeans on our Dataset with Conditionally Independent Views\n",
    "\n",
    "The co-Expectation Maximization framework (and co-training), relies on the fundamental assumption that data views are conditionally independent. If all views are informative and conditionally independent, then Multi-view KMeans is expected to produce higher quality clusters than Single-view KMeans, for either view or for both views concatenated together. Here, we will evaluate the quality of clusters by using the normalized mutual information metric, which is essentially a measure of the purity of clusters with respect to the true underlying class labels. <br>\n",
    "\n",
    "As we see below, Multi-view KMeans produces clusters with higher purity than Single-view KMeans across a range of values for the n_clusters parameter for data with complex and informative views, which is consistent with some of the results from the original Multi-view clustering paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-view View 1 NMI Score: 0.328\n",
      "\n",
      "Single-view View 2 NMI Score: 0.504\n",
      "\n",
      "Single-view Concatenated NMI Score: 0.428\n",
      "\n",
      "Multi-view NMI Score: 0.588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, labels = get_ci_data()\n",
    "m_clusters = perform_clustering(RANDOM_SEED, data, labels, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating an artificial dataset where the conditional independence assumption between views does not hold\n",
    "\n",
    "Here, we create an artificial dataset where the conditional independence assumption between\n",
    "views, given the true labels, is violated. We again derive our dataset from the forest covertypes \n",
    "dataset from sklearn. However, this time, we use only the first 3 classes of the dataset, which will\n",
    "correspond to the 3 clusters for view 1. To produce view 2, we will apply a simple nonlinear transformation to view 1 \n",
    "using the logistic function, and we will apply a negligible amount of noise to the second view to avoid convergence\n",
    "issues. This will result in a dataset where the correspondance between views is very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cd_data(num_samples=500):\n",
    "    \n",
    "    #Load in the vectorized news group data from scikit-learn package\n",
    "    cov = fetch_covtype()\n",
    "    all_data = np.array(cov.data)\n",
    "    all_targets = np.array(cov.target)\n",
    "    \n",
    "    #Set class pairings as described in the multiview clustering paper\n",
    "    view1_classes = [1, 2, 3]\n",
    "    view2_classes = [4, 5, 6]\n",
    "    \n",
    "    #Create lists to hold data and labels for each of the classes across 2 different views\n",
    "    labels =  [num for num in range(len(view1_classes)) for _ in range(num_samples)]\n",
    "    labels = np.array(labels)\n",
    "    view1_data = list()\n",
    "    view2_data = list()\n",
    "    \n",
    "    #Randomly sample 500 items from each of the selected classes in view1\n",
    "    for class_num in view1_classes:\n",
    "        class_data = all_data[(all_targets == class_num)]\n",
    "        indices = np.random.choice(class_data.shape[0], num_samples)\n",
    "        view1_data.append(class_data[indices])\n",
    "    view1_data = np.concatenate(view1_data)\n",
    "   \n",
    "        \n",
    "    #Construct view 2 by applying a nonlinear transformation\n",
    "    #to data from view 1 comprised of a linear transformation\n",
    "    #and a logistic nonlinearity\n",
    "    t_mat =  np.random.random((view1_data.shape[1], 50)) \n",
    "    noise = 0.005 - 0.01*np.random.random((view1_data.shape[1], 50))\n",
    "    t_mat *= noise\n",
    "    transformed = view1_data @ t_mat\n",
    "    view2_data = scp.special.expit(transformed)\n",
    "    \n",
    "    #Shuffle and normalize vectors\n",
    "    shuffled_inds = np.random.permutation(num_samples * len(view1_classes))\n",
    "    view1_data = np.vstack(view1_data)\n",
    "    view2_data = np.vstack(view2_data)\n",
    "    view1_data = view1_data[shuffled_inds]\n",
    "    view2_data = view2_data[shuffled_inds]\n",
    "    magnitudes1 = np.linalg.norm(view1_data, axis=0)\n",
    "    magnitudes2 = np.linalg.norm(view2_data, axis=0)\n",
    "    magnitudes1[magnitudes1 == 0] = 1\n",
    "    magnitudes2[magnitudes2 == 0] = 1\n",
    "    magnitudes1 = magnitudes1.reshape((1, -1))\n",
    "    magnitudes2 = magnitudes2.reshape((1, -1))\n",
    "    view1_data /= magnitudes1\n",
    "    view2_data /= magnitudes2\n",
    "    labels = labels[shuffled_inds]\n",
    "    return [view1_data, view2_data], labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the Performance of Multi-view and Single-view KMeans on our Dataset with Conditionally Dependent Views\n",
    "\n",
    "As mentioned before co-Expectation Maximization framework (and co-training), relies on the fundamental assumption that data views are conditionally independent. Here, we will again compare the performance of single-view and multi-view kmeans clustering using the same methods as before, but on our conditionally dependent dataset. <br>\n",
    "\n",
    "As we see below, Multi-view KMeans does not beat the best Single-view clustering performance with respect to purity, since that the views are conditionally dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-view View 1 NMI Score: 0.323\n",
      "\n",
      "Single-view View 2 NMI Score: 0.117\n",
      "\n",
      "Single-view Concatenated NMI Score: 0.195\n",
      "\n",
      "Multi-view NMI Score: 0.226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, labels = get_cd_data()\n",
    "m_clusters = perform_clustering(RANDOM_SEED, data, labels, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the performance of Multi-view and Single-view KMeans clustering on other complex data\n",
    "\n",
    "To see the relative performance of single-view and multi-view clustering on complex, real world data, please refer to the MultiviewKMeans_Tutorial notebook, which illustrates the application of both of these clustering methods to the UCI Digits Multiple Features Dataset. In this notebook, we can see that multi-view kmeans clustering produces clusters with higher purity than the single-view analogs when given informative views of data, even if conditional independence is not strictly enforced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
